job_id,job_title,company_name,descprition
4146066012,Staff Data Engineer - Data Science,LinkedIn,"LinkedIn is the world’s largest professional network, built to help members of all backgrounds and experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. LinkedIn’s Data Science team leverages big data to empower business decisions and deliver data-driven insights, metrics, and tools in order to drive member engagement, business growth, and monetization efforts. With over 1 billion members around the world, a focus on great user experience, and a mix of B2B and B2C programs, LinkedIn offers countless ways for an ambitious data engineer to have an impact and transform your career. We are now looking for a talented and driven individual to accelerate our efforts and be a major part of our data-centric culture. This person will work closely with various cross-functional teams such as product, marketing, sales, engineering, and operations to develop infrastructure and deliver tools or data structures that enable data-driven decision-making. Successful candidates will exhibit technical acumen and business savviness with a passion for making an impact by enabling both producers and consumers of data insight to work smarter. Responsibilities: • Work with a team of high-performing data science professionals, and cross-functional teams to identify business opportunities and build scalable data solutions. Build data expertise, act like an owner for the company and manage complex data systems for a product or a group of products. • Perform all of the necessary data transformations to serve products that empower data-driven decision making. • Establish efficient design and programming patterns for engineers as well as for non-technical partners. • Design, implement, integrate and document performant systems or components for data flows or applications that power analysis at a massive scale. • Ensure best practices and standards in our data ecosystem are shared across teams. • Understand the analytical objectives to make logical recommendations and drive informed actions. • Engage with internal data platform teams to prototype and validate tools developed in-house to derive insight from very large datasets or automate complex algorithms. • Contribute to engineering innovations that fuel LinkedIn’s vision and mission. Basic Qualifications: • Bachelor's Degree in a quantitative discipline: Computer Science, Statistics, Operations Research, Informatics, Engineering, Applied Mathematics, Economics, etc • 4+ years of relevant industry or relevant academia experience working with large amounts of data • Experience with SQL/Relational databases • Background in at least one programming languages (e.g., R, Python, Java, Scala, PHP, JavaScript) Preferred Qualifications: • BS and 7+ years of relevant work experience, MS and 5+ years of relevant work experience, or Ph.D. and 3+ years of relevant work/academia experience working with large amounts of data • MS or PhD in a quantitative discipline: Statistics, Operations Research, Computer Science, Informatics, Engineering, Applied Mathematics, Economics, etc. Suggested Skills : • Java • Distributed Systems • Relational Databases • Technical Leadership You will Benefit from our Culture: We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $147,000.00 to $240,000.00 Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice"
4124801858,Data Engineer,Adobe,"Our Company Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen. We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours! Be a part of Adobe’s Experience Platform; our fastest growing business in the Experience Cloud! The Adobe Experience Platform manages petabytes of data on behalf of organizations allowing them to have a centralized and standardized data platform applying data science and machine learning to improve the design and delivery of rich, personalized experiences. Adobe Experience Platform is seeking a Software Development Engineer to join the operational intelligence team. We build scalable, performant services and tools to handle end to end customer lifecycle from provisioning everything needed for onboarding customers when they purchase AEP to analyzing customer usage data and behaviors to generate business critical insights. We are looking for innovative and passionate software engineers to build low latency & highly scalable fault tolerant systems. What you will do: Design and develop distributed services that are resilient, highly available and scalable. Collaborate with business partners, architects, technical leads, product management and analysts to develop high-quality customer centric solutions. Participate in all aspects of software development activities, including design, coding, code review, unit and integration testing, bug fixing, deploy and code/API documentation. Own feature development from inception to production rollout and postmortem & contribute to the development of engineering processes. Help evaluate innovative technologies and incorporate them into our stack. What you will need to succeed: B.S. or M.S. in Computer Science or equivalent engineering degree. 3+ years of software engineering experience having built highly maintainable, scalable systems with Scala/Java or comparable strongly typed language. Experience with data transformation & ELT pipelines on large data sets using Databricks, SnowFlake, SQL, Python, Jupyter Notebooks. Excellent data analysis, problem-solving skills & proficiency with data visualization tools (e.g. Power BI, Tableau, Looker) Experience with building & deploying machine learning models & ML pipelines such as Sklearn, Tensorflow, PyTorch, KubeFlow, MLFlow, SageMaker, or similar. Ability to multi-task simultaneously different projects, having a positive outlook, motivated learner with strong interpersonal and written and verbal communication skills. What will help you stand out from the crowd Unending curiosity, thoroughness, tenacity and focus on designing and building complex software systems with excellent quality to address customer problems. Experience developing backend distributed applications on Java/JVM and Spring (or similar framework). Shown experience using structured, focused approaches to solving technical, data, and logical problems. Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $113,400 -- $206,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process. At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP). In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award. Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances. Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more. Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015. Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees."
4169661048,Data Engineer,Lattice,"This is Data Science at Lattice As a Data Engineer on our Data Science team, you will play a pivotal role in designing, building, and optimizing our data infrastructure to support data-driven decision-making across the organization. You will collaborate closely with data scientists, analysts, and other stakeholders to deliver high-quality data solutions that meet business needs. Your responsibilities will include writing production-level data transformations, managing ETL processes, and ensuring the performance and reliability of our data warehouse. With your expertise in SQL, Python, and data orchestration tools like Airflow, you will drive best practices and contribute to the velocity of our team and the continuous improvement of our tech stack. What You Will Do Write production-level data transformations to support stakeholders across data and business intelligence applications Create documentation and testing to ensure that our data is accurate and easily understandable Maintain and optimize performance of our data warehouse by designing efficient tables and storage processes Share methodologies and instruct other data team members on how to best write data transformations Collaborate with cross-functional teams to support data-driven initiatives and projects. Discover and institute best practices to ensure data quality across all facets of the data team Research and implement new tooling to enhance our tech stack Manage our ETL and other orchestrated processes through third party tools and first-party tools such as Airflow Enforce code quality by identifying and reducing tech debt, standardizing styles, and removing deprecated objects from code and downstream locations Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions What You Will Bring To The Table Fluency in SQL and dbt Proficiency in Python Experience with git and cloud platforms (e.g. AWS) Knowledge of data orchestration tools like Airflow, Prefect, or Dagster Excellent communication and collaboration skills High attention to detail and ability to think structurally about a solution Experience with a BI Tool (preferably Looker) A strong commitment to Lattice’s mission and values, including Ship Shipmate Self, Clear Eyes, Own The Outcome, and What’s Next? The estimated annual cash salary for this role is $123,000 - $154,000. This position is also eligible for incentive stock options, subject to the terms of Lattice’s applicable plans. Benefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: Medical insurance; Dental insurance; Vision insurance; Life, AD&D, and Disability Insurance; Emergency Weather Support; Wellness Apps; Paid Parental Leave, Paid Time off inclusive of holidays and sick time; Commuter & Parking Accounts; Lunches in the Office; Internet and Phone Stipend; One time WFH Office Set-Up Stipend; 401(k) retirement plan; Financial Planning; Learning & Development Budget; Sabbatical Program; and Invest in Your People Fund Note on Pay Transparency: Lattice provides an estimate of the compensation for roles that may be hired as required by state regulations. Compensation may vary based on (a) location, as Lattice factors in specific location when benchmarking compensation for most roles; (b) individual candidate skills and qualifications; and (c) individual candidate experience. Additionally, Lattice leverages current market data to determine compensation, so posted compensation figures are subject to change as new market data becomes available. The salary, other compensation, and benefits information is accurate as of the date of this posting. Lattice reserves the right to modify this information at any time, subject to applicable law. About Lattice Lattice is on a mission to build cultures where employees and their companies thrive. In an age where employees have more choices than ever before, businesses that put employees first are winning 🏅– and Lattice is building the tools to empower those people-centric companies. Lattice is a people success platform that offers performance reviews, employee engagement surveys, real-time feedback, weekly check-ins, goal setting, and career planning in a way that allows companies to focus on employee development, growth, and engagement – yielding stronger employee retention, performance, and impact to the bottom line 📈. Since launching in 2016, we have grown to over 5,000+ customers globally, including brands like Loom, Robinhood, and Gusto. Lattice is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Lattice is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation. By clicking the ""Submit Application"" button below, you consent to Lattice processing your personal information for the purpose of assessing your candidacy for this position in accordance with Lattice's Job Applicant Privacy Policy ."
4115022421,Data Engineer,Atlassian,"Overview Atlassian is looking for a Data Engineer to join our Data Engineering team and build world-class data solutions and applications that power crucial decisions throughout the organisation. We are looking for an open-minded, structured thinker who is passionate about building systems at scale. You will enable a world-class data engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights and play an active role in building Atlassian’s data-driven culture. Working at Atlassian Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company. Responsibilities Data is a BIG deal at Atlassian. We ingest over 180 billion events each month into our analytics platform and we have dozens of teams across the company driving their decisions and guiding their operations based on the data and services we provide. The data engineering team manages several data models and data pipelines across Atlassian, including finance, growth, product analysis, customer support, sales, and marketing. You'll join a team that is smart and very direct. We ask hard questions and challenge each other to constantly improve our work. As a Data Engineer, you will apply your technical expertise to build analytical data models that support a broad range of analytical requirements across the company. You will work with extended teams to evolve solutions as business processes and requirements change. You'll own problems end-to-end and on an ongoing basis, you'll improve the data by adding new sources, coding business rules, and producing new metrics that support the business. Qualifications Bachelor’s/Master's degree or equivalent in a STEM field with a minimum 2+ Years of Experience in Data Engineering or related field. Expertise in Python or other modern programming languages. Working knowledge of relational databases and query authoring via SQL. Experience designing data models for optimal storage, retrieval and dashboarding to meet product and business requirements. Experience building scalable data pipelines using Spark or Spark-SQL with Airflow scheduler/executor framework or similar scheduling tools. Experience building real-time data pipelines using a micro-services architecture. Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka). Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team. Well-versed in modern software development practices (Agile, TDD, CICD). Compensation At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are: Zone A: $140,100 - $186,800 Zone B: $126,100 - $168,200 Zone C: $116,300 - $155,100 This role may also be eligible for benefits, bonuses, commissions, and equity. Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter. Our Perks & Benefits Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more. About Atlassian At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together. We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines. To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them. To learn more about our culture and hiring process, visit go.atlassian.com/crh ."
4144480276,Data Engineer Jr.,CGI,"Position Description CGI is seeking a motivated individual who is passionate about helping the team and clients achieve excellence. Qualified candidates will have a creative, holistic, and systematic approach to problem-solving. The Data Engineer is responsible for assisting our client with building a robust data infrastructure to extract insights that enable data driven decision making. The successful candidate will have a broad understanding of computer and information science principles to build data pipelines using SQL, Python and PySpark. They will import and sync to various data sources, clean and standardize data elements, and fix data quality issues. They will collect, manage, and convert raw data into a usable format for the data scientists This position is located in our Arlington, VA office; however, a hybrid working model is acceptable for candidates within the National Capital Region. Due to the nature of work a Secret Security Clearance is required. Your future duties and responsibilities Creating and maintaining scalable data pipelines from multiple sources Collecting and storing data from multiple sources Building, maintaining, and optimizing data tables Coordinating with data scientists to ensure data infrastructure needs are met Communicating with leadership to articulate progress of ongoing data initiatives as well as any blockers. Building and maintaining processes to monitor and ensure data quality ensuring accurate information is fed into executive level dashboards Required qualifications to be successful in this role: Required Qualifications To Be Successful In This Role Bachelor's Degree in similar field. 2-3 years of experience in data integration and management Experience with SQL, Python and PySpark Excellent analytical and problem-solving skill Excellent oral and written communication skills Desired qualifications/non-essential skills required: Experience with Databricks or Data Lakes desired CGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to skill set, level, experience, relevant training, and licensure and certifications. To support the ability to reward for merit-based performance, CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for this role in the U.S. is $85,000.00 - $121,900.00. CGI Federal's benefits are offered to eligible professionals on their first day of employment to include: Competitive compensation Comprehensive insurance options Matching contributions through the 401(k) plan and the share purchase plan Paid time off for vacation, holidays, and sick time Paid parental leave Learning opportunities and tuition assistance Wellness and Well-being programs #CGIFederalJob Together, as owners, let’s turn meaningful insights into action. Life at CGI is rooted in ownership, teamwork, respect and belonging. Here, you’ll reach your full potential because… You are invited to be an owner from day 1 as we work together to bring our Dream to life. That’s why we call ourselves CGI Partners rather than employees. We benefit from our collective success and actively shape our company’s strategy and direction. Your work creates value. You’ll develop innovative solutions and build relationships with teammates and clients while accessing global capabilities to scale your ideas, embrace new opportunities, and benefit from expansive industry and technology expertise. You’ll shape your career by joining a company built to grow and last. You’ll be supported by leaders who care about your health and well-being and provide you with opportunities to deepen your skills and broaden your horizons. Come join our team—one of the largest IT and business consulting services firms in the world. Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status or responsibilities, reproductive health decisions, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics. CGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned. We make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members. All CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. Dependent upon role and/or federal government security clearance requirements, and in accordance with applicable laws, some background investigations may include a credit check. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances. CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI’s legal duty to furnish information."
4171303033,Data Engineer Intern,The Clorox Company,"Clorox is the place that’s committed to growth – for our people and our brands. Guided by our purpose and values, and with people at the center of everything we do, we believe every one of us can make a positive impact on consumers, communities, and teammates. Join our team. #CloroxIsThePlace Your role at Clorox: Join our dynamic team as a Data Engineer Intern! As a Data Engineer Intern at our organization, you will have the opportunity to work with cutting-edge technologies within the Azure stack. You’ll collaborate closely with the Finance Transformation Team, gain hands-on experience, and contribute to impactful data engineering projects. If you have a passion for data, analytics, and cloud technologies and are eager to develop your business data acumen, this role is perfect for you! In this role, you will: Key Responsibilities Quality Assurance (QA) Tasks: Assist in identifying and resolving QA bugs related to data pipelines and processes, ensuring data integrity and reliability. Validate data accuracy, completeness, and consistency by performing rigorous testing and analysis. Pipeline Engineering: Participate in designing, building, and maintaining data pipelines. Implement data transformations, data cleansing, and ETL processes to ensure high-quality data flow. Collaborate with senior engineers to enhance pipeline efficiency and performance. Small Feature Development: Take ownership of small features within the data pipeline. Write code to handle data ingestion, processing, and storage, ensuring seamless data integration. Independent Project: Lead a small independent project related to data engineering, choosing a topic of interest (e.g., optimizing pipeline performance, automating data validation). Present your findings and recommendations to the team. What we look for: Key Skills, Abilities, And Experience Required Education: Currently a Junior or Rising Senior in college pursuing a STEM degree (e.g., Computer Science, Engineering, Information Technology, etc.). Technical Skills: Proficiency in SQL, Python, and PySpark. Familiarity with Azure services such as Data Factory, Synapse, Databricks, and Azure Functions. Experience with data integration using APIs. Knowledge of data visualization tools (Power BI, Tableau). Analytical Skills: Strong problem-solving abilities with the capability to provide solutions and recommendations. Ability to document business requirements effectively in the form of Standard Operating Procedures (SOPs). Collaboration and Communication: Excellent teamwork and communication skills with cross-functional partners. Workplace type: We seek out and celebrate diverse backgrounds and experiences. We’re looking for fresh perspectives, a desire to bring your best, and a non-stop drive to keep growing and learning. At Clorox, we have a Culture of Inclusion. We believe our values-based culture connects to our purpose and helps our people be the best versions of themselves, professionally and personally. This means building a workplace where every person can feel respected, valued, and fully able to participate in our Clorox community. Learn more about our I&D program & initiatives here . Benefits we offer to help you be well and thrive: Competitive compensation Generous 401(k) program in the US and similar programs in international Health benefits and programs that support both your physical and mental well-being Flexible work environment, depending on your role Meaningful opportunities to keep learning and growing Half-day Fridays, depending on your location Please apply directly to our job postings and do not submit your resume to any person via text message. Clorox does not conduct text-based interviews and encourages you to be cautious of anyone posing as a Clorox recruiter via unsolicited texts during these uncertain times. To all recruitment agencies: Clorox (and its brand families) does not accept agency resumes. Please do not forward resumes to Clorox employees, including any members of our leadership team. Clorox is not responsible for any fees related to unsolicited resumes."
4115021577,Data Engineer,Atlassian,"Overview Atlassian is looking for a Data Engineer to join our Data Engineering team and build world-class data solutions and applications that power crucial decisions throughout the organisation. We are looking for an open-minded, structured thinker who is passionate about building systems at scale. You will enable a world-class data engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights and play an active role in building Atlassian’s data-driven culture. Working at Atlassian Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company. Responsibilities Data is a BIG deal at Atlassian. We ingest over 180 billion events each month into our analytics platform and we have dozens of teams across the company driving their decisions and guiding their operations based on the data and services we provide. The data engineering team manages several data models and data pipelines across Atlassian, including finance, growth, product analysis, customer support, sales, and marketing. You'll join a team that is smart and very direct. We ask hard questions and challenge each other to constantly improve our work. As a Data Engineer, you will apply your technical expertise to build analytical data models that support a broad range of analytical requirements across the company. You will work with extended teams to evolve solutions as business processes and requirements change. You'll own problems end-to-end and on an ongoing basis, you'll improve the data by adding new sources, coding business rules, and producing new metrics that support the business. Qualifications Bachelor’s/Master's degree or equivalent in a STEM field with a minimum 2+ Years of Experience in Data Engineering or related field. Expertise in Python or other modern programming languages. Working knowledge of relational databases and query authoring via SQL. Experience designing data models for optimal storage, retrieval and dashboarding to meet product and business requirements. Experience building scalable data pipelines using Spark or Spark-SQL with Airflow scheduler/executor framework or similar scheduling tools. Experience building real-time data pipelines using a micro-services architecture. Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka). Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team. Well-versed in modern software development practices (Agile, TDD, CICD). Compensation At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are: Zone A: $140,100 - $186,800 Zone B: $126,100 - $168,200 Zone C: $116,300 - $155,100 This role may also be eligible for benefits, bonuses, commissions, and equity. Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter. Our Perks & Benefits Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more. About Atlassian At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together. We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines. To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them. To learn more about our culture and hiring process, visit go.atlassian.com/crh ."
4115023391,Data Engineer,Atlassian,"Overview Atlassian is looking for a Data Engineer to join our Data Engineering team and build world-class data solutions and applications that power crucial decisions throughout the organisation. We are looking for an open-minded, structured thinker who is passionate about building systems at scale. You will enable a world-class data engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights and play an active role in building Atlassian’s data-driven culture. Working at Atlassian Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company. Responsibilities Data is a BIG deal at Atlassian. We ingest over 180 billion events each month into our analytics platform and we have dozens of teams across the company driving their decisions and guiding their operations based on the data and services we provide. The data engineering team manages several data models and data pipelines across Atlassian, including finance, growth, product analysis, customer support, sales, and marketing. You'll join a team that is smart and very direct. We ask hard questions and challenge each other to constantly improve our work. As a Data Engineer, you will apply your technical expertise to build analytical data models that support a broad range of analytical requirements across the company. You will work with extended teams to evolve solutions as business processes and requirements change. You'll own problems end-to-end and on an ongoing basis, you'll improve the data by adding new sources, coding business rules, and producing new metrics that support the business. Qualifications Bachelor’s/Master's degree or equivalent in a STEM field with a minimum 2+ Years of Experience in Data Engineering or related field. Expertise in Python or other modern programming languages. Working knowledge of relational databases and query authoring via SQL. Experience designing data models for optimal storage, retrieval and dashboarding to meet product and business requirements. Experience building scalable data pipelines using Spark or Spark-SQL with Airflow scheduler/executor framework or similar scheduling tools. Experience building real-time data pipelines using a micro-services architecture. Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka). Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team. Well-versed in modern software development practices (Agile, TDD, CICD). Compensation At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are: Zone A: $140,100 - $186,800 Zone B: $126,100 - $168,200 Zone C: $116,300 - $155,100 This role may also be eligible for benefits, bonuses, commissions, and equity. Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter. Our Perks & Benefits Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more. About Atlassian At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together. We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines. To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them. To learn more about our culture and hiring process, visit go.atlassian.com/crh ."
4115017957,Data Engineer,Atlassian,"Overview Atlassian is looking for a Data Engineer to join our Data Engineering team and build world-class data solutions and applications that power crucial decisions throughout the organisation. We are looking for an open-minded, structured thinker who is passionate about building systems at scale. You will enable a world-class data engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights and play an active role in building Atlassian’s data-driven culture. Working at Atlassian Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company. Responsibilities Data is a BIG deal at Atlassian. We ingest over 180 billion events each month into our analytics platform and we have dozens of teams across the company driving their decisions and guiding their operations based on the data and services we provide. The data engineering team manages several data models and data pipelines across Atlassian, including finance, growth, product analysis, customer support, sales, and marketing. You'll join a team that is smart and very direct. We ask hard questions and challenge each other to constantly improve our work. As a Data Engineer, you will apply your technical expertise to build analytical data models that support a broad range of analytical requirements across the company. You will work with extended teams to evolve solutions as business processes and requirements change. You'll own problems end-to-end and on an ongoing basis, you'll improve the data by adding new sources, coding business rules, and producing new metrics that support the business. Qualifications Bachelor’s/Master's degree or equivalent in a STEM field with a minimum 2+ Years of Experience in Data Engineering or related field. Expertise in Python or other modern programming languages. Working knowledge of relational databases and query authoring via SQL. Experience designing data models for optimal storage, retrieval and dashboarding to meet product and business requirements. Experience building scalable data pipelines using Spark or Spark-SQL with Airflow scheduler/executor framework or similar scheduling tools. Experience building real-time data pipelines using a micro-services architecture. Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka). Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team. Well-versed in modern software development practices (Agile, TDD, CICD). Compensation At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are: Zone A: $140,100 - $186,800 Zone B: $126,100 - $168,200 Zone C: $116,300 - $155,100 This role may also be eligible for benefits, bonuses, commissions, and equity. Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter. Our Perks & Benefits Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more. About Atlassian At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together. We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines. To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them. To learn more about our culture and hiring process, visit go.atlassian.com/crh ."
4172362992,Business Intelligence Data Engineer,TomoCredit,"Who We Are As seen on TechCrunch, Forbes, and Bloomberg, join one of fastest growing areas in FinTech by taking on the credit system. Work directly with one of Inc.’s top female founders and learn from some of the most talented people in the industry. Headquartered in San Francisco, Tomo’s mission is to replace the outdated credit system and open access to banking. We value passionate, down to earth, “can do” people who enjoy fine-tuning small details, without losing sight of the big picture. We are looking for someone who is driven to get things done and views obstacles as an exciting challenge that demands a creative solution. You are a self-starter with a high degree of rigor, organization, and discipline to get things done. Above all else, this role requires someone who takes great pride in their work and is inspired and motivated by their role in improving the way millions of people build their financial future. Job Summary: As a Business Intelligence Engineer / Analytics Engineer, you will be responsible for building and maintaining the data models, dashboards, and analytics pipelines that power our decision-making. You’ll collaborate with teams across product, engineering, operations, and finance to ensure we have reliable, scalable, and actionable data. Key Responsibilities: Data Infrastructure & Modeling: Design, build, and maintain scalable data models in our data warehouse (e.g., Snowflake, BigQuery, Redshift). BI & Reporting: Develop interactive dashboards and reports using tools like Looker, Tableau, or Power BI to provide key business insights. ETL & Data Pipelines: Create, optimize, and maintain ETL processes using dbt, Airflow, or similar tools. Data Quality & Governance: Ensure data accuracy, consistency, and integrity across all systems. Cross-Functional Collaboration: Work closely with product, engineering, finance, and other stakeholders to understand data needs and deliver insights. Advanced Analytics: Conduct deep-dive analyses to identify trends, opportunities, and areas for improvement. Automation & Optimization: Implement automation and performance improvements for reporting and analytics workflows. Qualifications: 3+ years of experience as a Business Intelligence Engineer, Analytics Engineer, or similar role. Hands-on experience with dbt (data modeling, transformations, testing). Strong SQL skills and experience with data modeling best practices. Experience with cloud data warehouses (e.g., Snowflake, BigQuery, Redshift). Proficiency with BI tools like Looker, Tableau, Power BI, or similar. Experience with ETL and orchestration tools like Airflow, Fivetran, or custom-built pipelines. Hands-on experience with Google Analytics, including data extraction, analysis, and reporting, to support cross-functional teams with actionable insights and data-driven decision-making. Understanding of data governance, security, and compliance best practices. Ability to communicate complex data concepts to non-technical stakeholders. Experience in fintech, credit, or financial services is a plus. Bonus: Familiarity with Python, R, or other data scripting languages. Why TomoCredit? Join a team where your work makes a significant impact on the future of credit and banking. At TomoCredit, you’ll collaborate with seasoned FinTech executives from Square, Lending Club, and American Express. We’re committed to fostering a culture where people love what they do and the team they work with. Here’s what we offer: Competitive Salary: Reflecting your skills and experience. Equity: We share our success with our employees through ownership stakes. Insurance: Comprehensive medical, dental, and vision benefits. Flexible Vacation Policy: We trust you to manage your time wisely to prevent burnout. Career Growth Opportunities: Take advantage of mentorship from seasoned professionals and expand your role as TomoCredit grows. Company-Sponsored Outings: Build meaningful relationships with your team outside of work. Commitment to Diversity At TomoCredit, diversity and inclusion are core to our values. We welcome applications from all qualified individuals, regardless of race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. Join us in reshaping the credit landscape and empowering millions to build a stronger financial future!"
4172879369,Data Engineer,Largeton Group,"Job Title: Senior Data Engineer Date: January 2025 Job Purpose: To work with Lead Data Engineers on building and maintaining data pipelines. Must be well-versed in data flow and support of analytical applications and have knowledge of modern programming languages, web technologies, backend infrastructure, databases, and AWS cloud services. Job Location: Hybrid, Nashville Department, Division: Technology, Global IT Reports to: VP, Data Warehouse & Analytics Manages Others: No Responsibilities: Collaborate with product manager, business analyst, and solutions architect Provide subject matter expertise Diagnose and remediate development issues Participate in code reviews Write efficient code Build reusable components and libraries Develop guidelines on code documentation Stay current with industry trends Promote teamwork Mentor junior members Requirements: 5 - 7+ years of Data Engineer experience Knowledge of modern database technologies and AWS Experience with unit and integration testing Strong documentation skills Knowledge of development life cycle phases and solution delivery for cloud-based systems Self-motivated and organized Ability to work across time zones (US and UK) Skills: AWS Serverless, Cloud Security, DevOps, Cloud Migration, Containers, Dockers Languages: SQL/NOSQL, Python, Node JS, Chart JS Knowledge of AWS Services like Event Bridge, Step Functions, Lambda, Glue, CloudWatch, CloudFront, DMS Knowledge of Python or PySpark, Liquibase, CI/CD Knowledge of Agile, SAFE, scrum, CI/CD, and DevOps Effective communication and interpersonal skills Nice to Haves: Knowledge of designing and building high volume serverless solutions in AWS Experience with Front End technologies like Angular/React JS Knowledge of ETL solutions such as Glue & Lambda functions Knowledge on dashboard platforms such as AWS QuickSight, Google Looker, Tableau, PowerBI Knowledge of big data technologies like AWS Redshift, Hive and Spark Experience with AWS Database Migration Service (DMS) Experience with CRM/Finance/Accounting systems AWS Certification."
4164193356,Data Engineer,Infosys,"Infosys is seeking a Senior Data Engineer. This position’s primary responsibility will be to provide technical expertise and coordinate for day-to-day deliverables for the team. The chosen candidate will assist in the technical design of large business systems; builds applications, interfaces between applications, understands data security, retention, and recovery. The role holder should be able to research on technologies independently to recommend appropriate solutions & should contribute to technology-specific best practices & standards; contribute to success criteria from design through deployment, including, reliability, cost-effectiveness, performance, data integrity, maintainability and scalability; contributes expertise on significant application components, program languages, databases, operating systems, etc., and guides/mentors the team during the build and test phases. Candidate must be located within commuting distance of Charlotte/Raleigh, NC or be willing to relocate to the area. This position may require travel to project locations. Required Qualifications Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education. At least 4 years of Information Technology experience. Experience in creating data engineering pipelines using Python with Object oriented concepts, Pyspark and AWS. Design, develop and implement new features to existing framework using PySpark and Python. Write efficient and effective standalone scripts in PySpark with transformations as per the defined business logic. Preferred Qualifications Work closely with the team to understand the requirements and develop solutions. Test and debug code to ensure it produces the desired results. Document all programming tasks and procedures for future reference and troubleshooting Use your expertise in Python and Object-oriented concepts to solve complex problems and implement robust solutions. Proficient in Python, PySpark, and Object-oriented programming concepts. Proven experience as a Senior Data Engineer or similar role. Strong problem-solving techniques with an ability to troubleshoot complex software issues. Experience with AWS is preferred, but not mandatory. Excellent communication skills, both written and verbal. Self-motivated and able to work independently with minimal supervision. Ability to work in team in diverse/ multiple stakeholder environment. Experience and desire to work in a Global delivery environment. Ability to learn and adapt to emerging technologies and enhance skills."
4157959045,Data Engineer Intermediate,TieTalent,"About It's fun to work in a company where people truly BELIEVE in what they are doing! Headquartered in Arvada, Colorado with operations and presence in Europe, the Middle East, India, Asia, Japan and China, Sundyne is a global manufacturer of precision-engineered, highly reliable, safe and efficient centrifugal pumps and compressors for use in chemical, petrochemical, hydrocarbon, hydrogen, pharmaceutical, power generation and industrial applications. Sundyne is a leader in delivering precision-engineered and highly reliable pumps & compressors to many of the world's most important markets, including energy, chemical, industrial, carbon capture, clean hydrogen, and renewable fuels. Sundyne pumps and compressors are available in API, ANSI/ASME, ISO and other industry compliant designs. To learn more about the Sundyne family of precision-engineered pumps and compressors, please visit www.sundyne.com. Position Description Sundyne is seeking an Intermediate Data Engineer on-site in our Arvada CO office, Remote or Remote Hybrid. Responsibilities will be to enhance our existing SSIS Data Analytics Cubes and DataSet solutions along with developing data mart and data warehouse and establish the roadmap for the next generation of Data/ML/AI tool sets. Support, Maintain and Enhance our Data warehouse solutions that provide our business leaders with the data and insights they need to make decisions and service our customers with excellence. Job Duties & Responsibilities The primary responsibility of the Data Engineer is to create and provide business value from corporate data assets by Building/supporting multi-dimensional & tabular data models using SSIS, SSRS and other Microsoft technologies to enhance our Data Warehouse Utilize SQL to curate and query large data sets, perform analyses, create custom views, and troubleshoot data anomaly issues back to the source systems. Maintain and enhance expertise across an application technology stack (Microsoft SQL) as well as a technology domain (BI/DW and Data Analytics) Design and map data models to shift raw data into meaningful insights and evaluate the effectiveness and capability of new data sources and data gathering techniques. Ability to embrace and leverage new technologies while working effectively with other information technology professionals and business users to ensure that the data solutions and platforms are stable, affective, efficient and provide value to business needs. Assist in leading the data management and governance processes with the strategic and operational definition, design, implementation, and continuous innovation. Also ensure Data Security, Privacy, Protection, Integrity, and Sensitivity Perform tests and validate all data flows and prepare all ETL processes according to business requirements and incorporate all business requirements into all design specifications. Documents all technical and system specifications documents for all ETL processes and perform unit tests on all processes and prepare required programs and scripts. Develop strong data documentation about algorithms, parameters, models, data flows, and Transformation. Ensure accuracy of data thru testing and Business QA processes Skills & Abilities Degree in data engineering, Computer Science, Statistics, or related fields within advanced data and analytics 6 + years Data Engineer using MS SQL Server, TSQL and SSIS Must understand Relational and Dimensional database modeling. Must exhibit a deep understanding and knowledge of the Microsoft BI Stack, including: SSIS, SSRS, SSAS, SharePoint, PowerPivot, Power Query, MDX, DAX, and PowerBI Advanced experience using MS SQL to extract data, understand data structures, run queries, and analyze data in a data warehouse environment Strong understanding of tier one Manufacturing ERP System data (JDE, SAP, Syteline, Navision) Must have proven experience in ETL and experience in Tools to support Data Migration from one or more Source(s) to Destination with application of transformation rules. Effective Communication skills, both written and verbal Highly proficient with Microsoft Office Suite: Excel (advanced features), Word, Outlook, Power Point, Teams, Share Point Strong communication skills and the ability to synthesize findings from analyses into clear, concise communications for business leadership Team player attitude and strong ability to collaborate with colleagues across the globe Additional Skill Sets A Plus Microsoft Dynamics CRM System data RDBMS skills with dimensional modelling (SSMS) Azure Data lakes and AI/ML feature sets. Azure Databricks, Azure Data Factory, Azure Synapse, Fabric Python, Power Shell If you like growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us! Compensation Details Annual Salary: $120,000.00 - $125,000.00 Additional Compensation The Salary offered will be determined based on the applicant's education, experience, skills, knowledge, abilities, and will be compared with internal equity along with market data for this position. This position may be eligible to participate in the Company's Sales Incentive Plan with a target of up to 30% of base salary, subject to the terms and conditions of the plan. Payment is based on individual performance against established sales targets. This position may also be eligible to receive a Relocation bonus, payable as a taxable lump sum, in accordance with the Sundyne Relocation Policy. Application Deadline: 2025-04-09 Nice-to-have skills SSIS SQL ETL SAP Azure Data Factory Python Colorado, United States Work experience Data Engineer Data Infrastructure Languages English"
4171303033,Data Engineer Intern,The Clorox Company,"Clorox is the place that’s committed to growth – for our people and our brands. Guided by our purpose and values, and with people at the center of everything we do, we believe every one of us can make a positive impact on consumers, communities, and teammates. Join our team. #CloroxIsThePlace Your role at Clorox: Join our dynamic team as a Data Engineer Intern! As a Data Engineer Intern at our organization, you will have the opportunity to work with cutting-edge technologies within the Azure stack. You’ll collaborate closely with the Finance Transformation Team, gain hands-on experience, and contribute to impactful data engineering projects. If you have a passion for data, analytics, and cloud technologies and are eager to develop your business data acumen, this role is perfect for you! In this role, you will: Key Responsibilities Quality Assurance (QA) Tasks: Assist in identifying and resolving QA bugs related to data pipelines and processes, ensuring data integrity and reliability. Validate data accuracy, completeness, and consistency by performing rigorous testing and analysis. Pipeline Engineering: Participate in designing, building, and maintaining data pipelines. Implement data transformations, data cleansing, and ETL processes to ensure high-quality data flow. Collaborate with senior engineers to enhance pipeline efficiency and performance. Small Feature Development: Take ownership of small features within the data pipeline. Write code to handle data ingestion, processing, and storage, ensuring seamless data integration. Independent Project: Lead a small independent project related to data engineering, choosing a topic of interest (e.g., optimizing pipeline performance, automating data validation). Present your findings and recommendations to the team. What we look for: Key Skills, Abilities, And Experience Required Education: Currently a Junior or Rising Senior in college pursuing a STEM degree (e.g., Computer Science, Engineering, Information Technology, etc.). Technical Skills: Proficiency in SQL, Python, and PySpark. Familiarity with Azure services such as Data Factory, Synapse, Databricks, and Azure Functions. Experience with data integration using APIs. Knowledge of data visualization tools (Power BI, Tableau). Analytical Skills: Strong problem-solving abilities with the capability to provide solutions and recommendations. Ability to document business requirements effectively in the form of Standard Operating Procedures (SOPs). Collaboration and Communication: Excellent teamwork and communication skills with cross-functional partners. Workplace type: We seek out and celebrate diverse backgrounds and experiences. We’re looking for fresh perspectives, a desire to bring your best, and a non-stop drive to keep growing and learning. At Clorox, we have a Culture of Inclusion. We believe our values-based culture connects to our purpose and helps our people be the best versions of themselves, professionally and personally. This means building a workplace where every person can feel respected, valued, and fully able to participate in our Clorox community. Learn more about our I&D program & initiatives here . Benefits we offer to help you be well and thrive: Competitive compensation Generous 401(k) program in the US and similar programs in international Health benefits and programs that support both your physical and mental well-being Flexible work environment, depending on your role Meaningful opportunities to keep learning and growing Half-day Fridays, depending on your location Please apply directly to our job postings and do not submit your resume to any person via text message. Clorox does not conduct text-based interviews and encourages you to be cautious of anyone posing as a Clorox recruiter via unsolicited texts during these uncertain times. To all recruitment agencies: Clorox (and its brand families) does not accept agency resumes. Please do not forward resumes to Clorox employees, including any members of our leadership team. Clorox is not responsible for any fees related to unsolicited resumes."
4164169385,Data Engineer,WHOOP,"At WHOOP, we're on a mission to unlock human performance. WHOOP empowers members to perform at a higher level through a deeper understanding of their bodies and daily lives. WHOOP is seeking a dynamic Data Engineer who thrives on innovation and is ready to revolutionize our data operations. In this role, you'll design, build, and optimize scalable data pipelines and platforms that serve as the backbone of our data-driven insights. With a strong focus on crafting robust ETL/ELT processes and managing cutting-edge AWS infrastructure, you'll integrate modern data tools—including Snowflake, DBT, Kafka, and Spark—to elevate our analytical capabilities. If you're excited about harnessing AI to supercharge efficiency and drive breakthrough innovations, we want you to join our forward-thinking team and make a tangible impact on the future of our data ecosystem. Responsibilities Write and maintain high-quality, reusable code in Python and Pyspark to develop and maintain ELTs and data pipelines. Utilize Kafka and Spark for real-time streaming and batch data processing. Implement and optimize data warehousing solutions using Snowflake. Create and manage transformation models with DBT to ensure consistent data quality and agile analytics. Architect and manage AWS infrastructure (e.g., EC2, S3, Lambda, RDS) to support scalable and secure data processing. Leverage AI tools to automate tasks, optimize workflows, and drive overall efficiency. Collaborate with cross-functional teams (data scientists, analysts, etc.) to understand and meet evolving data needs. Document processes and continuously seek improvements in the data platform. Qualifications Bachelor’s degree in Computer Science, Engineering, or a related field; or equivalent practical experience. Experience designing and implementing ETL/ELT processes. Solid understanding of SQL and modern data warehousing concepts. Familiarity with AI tools like Copilot / ChatGPT and their use on driving efficiency in the software development life cycle. Proficiency using DBT for data modeling and transformation is a plus. Hands-on experience with Kafka and Spark for data processing is a plus. Knowledge of containerization, orchestration (e.g., Docker, Kubernetes), and infrastructure-as-code is a plus. Excellent problem-solving skills, strong communication abilities, and the capacity to work collaboratively in an agile environment. This role is based in the WHOOP office located in Boston, MA. The successful candidate must be prepared to relocate if necessary to work out of the Boston, MA office. Interested in the role, but don’t meet every qualification? We encourage you to still apply! At WHOOP, we believe there is much more to a candidate than what is written on paper, and we value character as much as experience. As we continue to build a diverse and inclusive environment, we encourage anyone who is interested in this role to apply. WHOOP is an Equal Opportunity Employer and participates in E-verify to determine employment eligibility. It is unlawful in Massachusetts to require or administer a lie detector test as a condition of employment or continued employment. An employer who violates this law shall be subject to criminal penalties and civil liability."
4154582788,Data Engineer,FanDuel,"About Fanduel FanDuel Group is the premier mobile gaming company in the United States. FanDuel Group consists of a portfolio of leading brands across mobile wagering including, America’s #1 Sportsbook FanDuel Sportsbook, its leading iGaming platform FanDuel Casino, the industry’s unquestioned leader in horse racing and advance-deposit wagering, FanDuel Racing and its daily fantasy sports product. In addition, FanDuel Group operates FanDuel TV, its broadly distributed linear cable television network and FanDuel TV+, its leading direct-to-consumer OTT platform. FanDuel Group has a presence across all 50 states and Puerto Rico with approximately 17 million customers and 31 retail locations. The company is based in New York with offices in Los Angeles, Atlanta and Jersey City, as well as in Canada, Scotland, Ireland, Portugal, Romania and Australia. FanDuel Group is a subsidiary of Flutter Entertainment, the world's largest sports betting and gaming operator with a portfolio of globally recognized brands and traded on the New York Stock Exchange (NYSE: FLUT). THE ROSTER At FanDuel, we give fans a new and innovative way to interact with their favorite games, sports and teams. We’re dedicated to building a winning team and we pride ourselves on being able to make every moment mean more, especially when it comes to your career. So, what does “winning” look like at FanDuel? It’s recognition for your hard-earned results, a culture that brings out your best work—and a roster full of talented coworkers. Make no mistake, we are here to win, but we believe in winning right. That means we’ll never compromise when it comes to looking out for our teammates. From creatives professionals to cutting edge technology innovators, FanDuel offers a wide range of career opportunities, best in class benefits, and the tools to explore and grow into your best selves. At FanDuel, our principle of “We Are One Team” runs through all our offices across the globe, and you can expect to be a part of an exciting company with many opportunities to grow and be successful. THE POSITION Our roster has an opening with your name on it FanDuel is looking for an experienced Data Engineer with a deep understanding of large-scale data handling and processing best practices in a cloud environment to help us build scalable systems. As our data is a key component of the business used by almost every facet of the company, including product development, marketing, operations, and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability. Our competitive edge comes from making decisions based on accurate and timely data and your work will provide access to that data across the whole company. Looking ahead to the next phase of our data platform we are keen to do more with real time data processing and working with our data scientists to create machine learning pipelines We're seeking dynamic professionals to join our business-critical Risk and Trading data team at FanDuel, where you'll harness cutting-edge technologies to support risk analysis, pricing models, and trading operations. Collaborate with a broad array of Risk, Trading and Fraud stakeholders and deploy crucial data assets to advance operational and machine learning use cases. THE GAME PLAN Everyone on our team has a part to play Creating and maintain optimal data pipelines Implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologies. Identifying and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability Deploying data models and views with large datasets that meet functional / non-functional business requirements Delivering data integration solutions to downstream marketing and campaign software Delivering quality production-ready code in an agile environment Delivering test plans, monitoring, debugging and technical documents as a part of development cycle Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs Adhere to engineering and operational excellence THE STATS What we're looking for in our next teammate 2-4+ years of experience writing Python, SQL and DBT Proficiency in relational databases and data warehouses (Delta Lake knowledge is a plus) Show proficiency understanding complex ETL processes Understanding of Risk and/or Fraud terminology Knowledge of data integrity and relational rules Understanding of AWS Ability to quickly learn new technologies is critical Proficiency with agile or lean development practices Player Benefits We treat our team right From our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect: An exciting and fun environment committed to driving real growth Opportunities to build really cool products that fans love Career and professional development resources to help you refine your game plan for owning and driving your career and development Be well, save well and live well - with FanDuel Total Rewards your benefits are one highlight reel after another FanDuel is an equal opportunities employer and we believe, as one of our principal states, “We Are One Team!” We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, Veteran status, or another other characteristic protected by state, local or federal law. We believe FanDuel is strongest and best able to compete if all employees feel valued, respected, and included. We want our team to include diverse individuals because diversity of thought, diversity of perspectives, and diversity of experiences leads to better performance. Having a diverse and inclusive workforce is a core value that we believe makes FanDuel stronger and more competitive as One Team! The applicable salary range for this position is $108,000 - $142,000, which is dependent on a variety of factors including relevant experience, location, business needs and market demand. This role may offer the following benefits: medical, vision, and dental insurance; life insurance; disability insurance; a 401(k) matching program; among other employee benefits. This role may also be eligible for short-term or long-term incentive compensation, including, but not limited to, cash bonuses and stock program participation. This role includes paid personal time off and 14 paid company holidays. FanDuel offers paid sick time in accordance with all applicable state and federal laws."
4132851002,Data Engineer,Paradigm,"At Paradigm, we are changing the future of finance! By joining us at this early stage, you’ll be building cutting-edge, distributed financial service infrastructure that will reshape financial services across CeFi and DeFi markets. About Paradigm Paradigm is a zero-fee, institutional liquidity network for derivatives traders across CeFi and DeFi. We provide unified access to multi-asset, multi-protocol liquidity on demand without compromising on execution preferences, costs, and immediacy. We’ve built the largest network of institutional counterparties in crypto, with over 1000 institutional clients trading over $10 B per month. We are a diverse, global team led by our organizational principles and united by our mission to bring on-demand liquidity for traders, anytime and anywhere, without compromises. We also strive to ship faster than anyone else in the industry! We are backed by the best traders and investors in the space, including Jump Capital, Genesis Trading, Dragonfly Capital, QCP Capital, Optiver US, IMC, GSR Markets, Akuna Capital, Fidelity Digital Assets CMT Digital, Goldentree Asset Management, Amber Group, OK Group, Bybit Fintech, and CoinShares. Your Mission As a vital member of our data engineering team, you’ll architect and implement robust data infrastructure that transforms raw data into actionable insights. You’ll work on building scalable pipelines and data models that power critical decision-making across the organization. Your expertise will be crucial in establishing data best practices and ensuring data quality, accessibility, and reliability throughout our systems. What You’ll Do Design & Build: Create and maintain scalable data pipelines, ETL processes, and data warehousing solutions that handle complex data requirements. Optimize & Scale: Improve data infrastructure performance, implement data quality measures, and develop automated monitoring systems. Empower & Enable: Partner with product, engineering and go-to-market teams to deliver data solutions that drive strategic insights. Rapid Response: Quickly address time-sensitive data needs and ad-hoc requests from stakeholders, turning around critical analyses and data solutions with urgency while maintaining accuracy. What You’ll Bring Expertise: 7+ years of data engineering experience Data Architecture: Deep knowledge of data modeling, warehouse design, and ETL best practices. Technical Mastery: Proficiency with modern data stack (Snowflake, Airflow/DBT) and AWS. Systems Thinking: Experience with distributed systems, data streaming (Kafka/Kinesis), and optimization of large-scale data workflows. Agility: Proven track record of efficiently tackling urgent data requests and providing quick solutions to business-critical data needs. salary range: USD 142,000 to 237,000 At Paradigm we're doing something different. Moving forward we ask all candidates to send in a video with their application, telling us why they want to be part of the team. The video should be no longer than 1 minute and via a link to a streaming platform of your choice. (No files or download links will be accepted) Your Perks & Benefits Competitive Pay: Top-tier compensation in the industry. Generous PTO: Unlimited vacation. Full Benefits: Comprehensive packages tailored by country. Technology & Learning Allowances: 3,500 USD for your first-year setup, $2,000 USD refresh every 2 years, plus $1,000 USD annually for learning and development. Paradigm is an equal opportunity employer."
4138463912,Data Engineer,Suno,"About Suno At Suno, we are building a future where anyone can make music. You can make a song for any moment with just a few short words. Award-winning artists use Suno, but our core user base consists of everyday people making music — often for the first time. We are a team of musicians and AI experts, including alumni from Spotify, TikTok, Meta and Kensho. We like to ship code, make music and drink coffee. Our company culture celebrates music and experimenting with sound — from lunchroom conversations to the studio in our office. About The Role We’re seeking talented data engineers to join our founding team, working closely with key stakeholders and taking ownership of building and shaping Suno’s core data foundation. Check out our Suno version of the job here! What You’ll Do Serve as a critical contributor to the Suno products team, providing insights and solutions that influence high-level decisions and shape the product roadmap. Design, develop, and maintain complex data products, systems, platforms, and pipelines to build scalable, secure, and high-quality big data solutions that seamlessly integrate diverse data sources, process high-volume real-time and batch data, and ensure data integrity, quality, and compliance. Collaborate with scientists, ML engineers, software developers, business leaders, and product teams to define data requirements, architect solutions, and implement data-driven opportunities that enhance customer experiences. Leverage advanced analytics and data engineering practices to uncover actionable customer insights that drive the development of innovative and enhanced customer experiences for Suno. Implement and monitor data systems to ensure high availability, reliability, and scalability while advocating for and applying best practices in big data engineering and analytics. What You’ll Need 5+ years of experience in data engineering, with a strong grasp of Big Data engineering concepts, data architecture design, and performance optimization. Experience with data modeling, warehousing and building ETL pipelines. Experience with SQL and Python. Experience in scaling data pipelines from the ground up (0 to 1) Familiarity with Airflow, DBT, and Snowflake is a strong advantage. Passionate about engineering excellence, rapid iteration, learning, and hard work. Technical leadership or management experience is a plus. A love of music (listening, exploring, making) is a huge plus. Additional Notes: Applicants must be eligible to work in the US. Compensation The annual salary/OTE range for the target level for this role is $170,000 - $240,000 + target equity + benefits (including medical, dental, vision, and 401k) Benefits Healthcare for you and your dependents, with vision and dental 401k with match Generous commuter benefit Flexible PTO"
4113934016,Data Engineer,PayPal,"The Company PayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy. We operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers. We offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade. Our beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do – and they push us to ensure we take care of ourselves, each other, and our communities. Job Description Summary: As a Data Engineer on the Credit Platform Data team at PayPal, you'll play a key role in building and enhancing tools for data processing, enabling our internal business units to leverage data efficiently. You'll develop secure, high-performing data integration and ETL processes, participating in all stages of development from, analysis to production releases. Your responsibilities will include delivering new features, ensuring quality, and collaborating closely with the Product team in an agile environment. Job Description: As a Data Engineer on the Credit Platform Data team at PayPal, you'll play a key role in building and enhancing tools for data processing, enabling our internal business units to leverage data efficiently. You'll develop secure, high-performing data integration and ETL processes, participating in all stages of development from analysis to production releases. Your responsibilities will include delivering new features, ensuring quality, and collaborating closely with the Product team in an agile environment. Responsibilities: Design, build, and maintain robust data pipelines and ETL processes to ingest, transform, and load data from various sources into our data warehouse, ensuring scalability and efficiency. Collaborate with product managers, analysts, and other stakeholders to understand data requirements and develop solutions that meet business needs while accommodating large volumes of data. Ensure the reliability, availability, and scalability of our data systems, monitoring performance and optimizing as needed to handle increasing data volumes. Implement automated data quality checks and validation processes to ensure data integrity and accuracy at scale. Troubleshoot data-related issues, identify root causes, and implement solutions in a timely manner to minimize impact on data processing. Create and maintain design documents and documentation for data pipelines, systems, and processes. Participate actively in design and code reviews. Stay current with emerging technologies and trends in data engineering, recommending and implementing improvements as necessary to support scalability and growth. Qualifications: Bachelor's degree in Computer Science, Engineering, or a related field. 3+ years of proven experience as a Data Engineer or similar role, with a strong background in database development, ETL processes, and software development. Proficiency in SQL and scripting languages such as Python, with experience working with relational databases. Proficiency in PySpark, Pandas or other data processing libraries. Familiarity with data warehousing concepts and tools, such as AWS Redshift, Google BigQuery, or Snowflake, and experience optimizing performance for large-scale data processing. Experience with data modeling, schema design, and optimization techniques for scalability. Strong analytical and problem-solving skills, with the ability to troubleshoot complex data issues and optimize data processing pipelines for scale. Experience with AWS or GCP as well as experience with Unix/Linux operating systems and shell scripting. Excellent communication and collaboration skills, with the ability to work effectively in a team environment. Self-motivated and proactive, with a passion for continuous learning and professional development. For the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations. Our Benefits: At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you. We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com Who We Are: To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx Commitment to Diversity and Inclusion PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com. Belonging at PayPal: Our employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal. Any general requests for consideration of your skills, please Join our Talent Community. We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don’t hesitate to apply. REQ ID R0121691"
4138522109,Data Engineer,Landbase,"Landbase leverages the experience of 100+ world class sales professionals and AI to deliver targeted, high-quality leads on autopilot. Our mission is to achieve GTM automation so humans no longer need to work for their software so they can reclaim their day. We're building GTM-1 Omni - the world's first action model purpose built for lead generation. About The Role We're seeking a Data Engineer to design and implement scalable data pipelines that power our AI models and analytics. Required Skills & Experience Design and implement ETL pipelines in PySpark and/or Scala Spark Orchestrate complex data workflows in Airflow Advanced SQL skills and experience with BigQuery Hands-on experience with MongoDB, Elasticsearch, and PostgreSQL Proficiency with GCP Dataproc and cluster optimization Experience handling TB-scale unstructured data Optimize database performance and storage strategies Create and maintain data quality monitoring systems Collaborate with ML Engineers on production model deployment Stay current with emerging AI tools and technologies 3+ years of experience in large scale data engineering Nice-to-Have Qualifications Experience with PyTorch, Hugging Face, or other ML frameworks Background in LLM operations and deployment Familiarity with Ray for distributed computing Experience with real-time data streaming architectures Background in sales/marketing data systems Previous experience in startup or AI-driven environments Benefits & Perks Competitive compensation Comprehensive health benefits Flexible work arrangements Professional development opportunities Exciting work with cutting-edge AI technology Collaborative and innovative work environment Regular team events and gatherings"
4173850433,Junior Data Engineer (Remote),SynergisticIT,"Since 2010 SynergisticIT has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers. In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. Please Check The Below Links All Positions are open for all visas and US citizens We at SynergisticIT understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients. Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry. We assist in filing for STEM extension and also for H1b and Green card filing to Candidates We want Data Science/Machine learning/Data Analyst and Java Full stack candidates For data Science/Machine learning Positions Required Skills Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Project work on the technologies needed Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools Excellent written and verbal communication skills Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow REQUIRED SKILLS For Java /Full Stack/Software Positions Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Project work on the skills Knowledge of Core Java, javascript, C++ or software programming Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience Excellent written and verbal communication skills If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements. No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"
4143664964,Data Engineer,Chubb,"Job Description Document Ingestion is a top priority for Chubb. We are looking for an innovative, results-oriented Data Engineer who has an ambition to contribute to the success of our Document Ingestion program. The successful candidate will be self-motivated, creative, and they will be able to independently contribute to our major strategic initiative. You will be responsible for contributing to the development and launch of solution across the Domain and/or Platform, supporting our overall strategy by helping define product features, helping build system architecture, and spearheading the best practices that enable a quality product. The candidate is expected to work in a Agile development environment, building high quality software that adds business value to the organization. The candidate with be part of a geographically dispersed organization working towards a common goal. The ideal candidate should have Knowledge of object-oriented programming, analysis, design and development (OOA/OOD) and the Python and strong data structures understanding, with more than 3 years of experience. Activities Contribute to the development and launch of solutions across the Domain and/or Platform, assisting in defining product features, building system architecture, and championing best practices for a quality product. Collaborate with business stakeholders to translate their requirements into technical stories and software solutions. Participate in solutioning and estimation activities for upcoming opportunities. Work with a team of software developers to build high-quality software solutions that align with architecture guidelines. Support technical solutioning and troubleshooting, collaborating closely with Tech Leads and Squads. Collaborate with Platform teams to improve pluggable and reusable ingestion components. Assist in the development of an analysis and design methodology, ensuring an agile implementation process that adheres to established standards and release cycles. Qualifications 3+ years of software engineering or data engineering, technology or insurance domain experience or a combination of both. Education in business, data, engineering, mathematics or information management Strong experience with Python, understanding of Java based microservices. SOAP web services, REST APIs, Kafka, MQ, Git, TFS and/or DevOps tools Unit and integration testing using Mockito. Knowledge on CICD Jenkins pipeline and JIRA and experience in Version Control tools like GIT/SVN Experience developing software/automation solutions using Python. Experience with Cloud Computing, Azure, AKS, Python and Microservices Good SQL and familiarity with relational databases Exposure to DevOps, CI/CD (Jenkins, Jira etc.) Comfort in a dynamic and fast-moving work environment. Excellent verbal, written & interpersonal communication skills. Proven ability as a structured, logical thinker Insurance domain or financial services in general The pay range for the role is $96,300 to $163,700. The specific offer will depend on an applicant’s skills and other factors. This role may also be eligible to participate in a discretionary annual incentive program. Chubb offers a comprehensive benefits package, more details on which can be found on our careers website . The disclosed pay range estimate may be adjusted for the applicable geographic differential for the location in which the position is filled. About Us Chubb is a world leader in insurance. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally. At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment."
4158257670,"Data Engineer (Python, SQL, Tableau)",Dexian,"Job Summary: Dexian is seeking a Data Engineer (Python, SQL, Tableau) for an opportunity with a client located in Charlotte, NC. Responsibilities: Write Python and/or PySpark code to automate production processes of several risk and loss measurement statistical models. Example of model execution production processes are error attribution, scenario shock, sensitivity, result publication and reporting Work closely with application users, model developers, technology and other business partners to understand and document requirements of production processes to be implemented Work with technology teams to integrate Python solution into existing in-house generic platform for process execution Ensure that software is developed to meet functional, non-functional, and compliance requirements Write complex SQL queries to validate production results, integrate results with existing downstream applications and produce reports in format expected by end users Requirements: 4+ years of Python programming experience, preferably with Apache spark or distributed computing experience (Spark, MapReduce, DataFrame, Spark SQL) with a Masters or PhD in an analytical field (e.g., Economics, Mathematics, Engineering) Knowledge of database and SQL language for Oracle, and Hive/Impala Understanding code development process steps using SDLC tools: PyCharm & Git Strong troubleshooting and debugging skills in Python Strong Collaboration and Team skills Demonstrated problem solving skills Effective communicator at all levels with strong oral, written & influencing skills Integrates seamlessly across complex set of stakeholders, internal partners, external resources Is flexible, adaptive, and proactive Knowledge of Banking and Finance domain and/or experience working with model developers Experience with Agile Development, and/or Test Driven Development Dexian is a leading provider of staffing, IT, and workforce solutions with over 12,000 employees and 70 locations worldwide. As one of the largest IT staffing companies and the 2nd largest minority-owned staffing company in the U.S., Dexian was formed in 2023 through the merger of DISYS and Signature Consultants. Combining the best elements of its core companies, Dexian's platform connects talent, technology, and organizations to produce game-changing results that help everyone achieve their ambitions and goals. Dexian's brands include Dexian DISYS, Dexian Signature Consultants, Dexian Government Solutions, Dexian Talent Development and Dexian IT Solutions. Visit https://dexian.com/ to learn more. Dexian is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status."
4132964570,Data Engineer (Healthcare Data),Pangaea Data,"About Pangaea Data Pangaea Data (Pangaea) is a South San Francisco and London based business founded by Dr Vibhor Gupta and Prof Yike Guo (Director Data Science Institute at Imperial College London; Provost, Hong Kong University of Science and Technology). They have worked in medicine and computing for over 20 years and have raised over $300 million through their academic research, including a $110 million grant focused on development work on large language models in medicine. Pangaea’s AI platform, PALLUX, is configured on clinical guidelines to find more untreated (undiagnosed, miscoded, at-risk) and under-treated patients with hard-to-diagnose conditions for screening and treatment at the point of care. Pangaea’s advisors include industry veterans from healthcare and the life sciences, including Lord David Prior (former chairman, NHS England) and Mr. Andy Palmer (former CIO, Novartis). The Role As Data Engineer (Healthcare Data), you will join Pangaea’s team to lead and support the development of reliable, scalable, and secure data solutions. The ideal candidate will be experienced with healthcare data standards (e.g. FHIR, OMOP), possess a strong understanding of data privacy regulations (e.g., HIPAA, GDPR), and have technical expertise to design and implement data pipelines, storage systems, and integrations. This role will continue to evolve as the business grows, but in the short term it will also involve development of the software product and collaboration with the clinical and scientific team. A strong software engineering background and knowledge in AI, especially Machine Learning and Natural Language Processing, is essential. For the right candidate, this is a senior technical position with scope to grow into a leadership role. Key Technical Responsibilities Will Include Design, implement, and maintain ETL pipelines to collect, clean, and transform healthcare data from various sources such as EHR systems, APIs, and databases Ensure data quality and integrity through robust testing and validation processes Optimize storage solutions for structured and unstructured healthcare data using databases (e.g., MongoDB) and cloud-based data warehouses (e.g., Azure Cosmos, Azure Fabric) Maintain strict compliance with data privacy regulations such as HIPAA, GDPR, and other local healthcare policies Work closely with the clinical team to understand data requirements and translate them into technical solutions Collaborate with the AI team to provide clean, well-structured datasets for research, and AI/ML models Stay up-to-date with the latest data engineering technologies and best practices Mandatory Requirements Technical Skills Experience working with Electronic Health Records (EHR) systems (e.g. Epic, Cerner) A university qualification (Bachelors, Masters, Doctorate) with at least two years of university study in Computer Science, Informatics, Data Science, Engineering, or related Experience in data engineering, with a focus on healthcare data preferred Familiarity with NoSQL databases (e.g., MongoDB) and relational databases (e.g., PostgreSQL, MySQL) 5+ years in Python and SQL work Knowledge of ETL tools (e.g., Apache Airflow) and cloud platforms (e.g., AWS, Azure, GCP). Understand data modelling concepts and best practices. Experience with healthcare data standards (e.g., HL7, FHIR, ICD, SNOMED, DICOM) preferred Excellent problem-solving and communication skills Personal Traits Ability to communicate complex ideas effectively, both verbally and written Ability to engage all levels of the company and the customers’ organizations Ability to work collaboratively in a team environment Nice to Have 3-5 years experience of managing teams Experience working on large-scale, commercial software development projects is a plus Experience with research communities and/or efforts, including having published papers (being listed as author) at AI/ML/NLP/CV conferences (e.g. Bio-IT, NeuraIPS, ICML, ICLR, ACL, CVPR and KDD) and journals Experience and knowledge of deploying AI and Data solutions for healthcare and pharmaceuticals at scale is desirable Perks and Benefits Flexible working hours Salary dependent on experience Package of attractive benefits including private medical insurance and monthly travel card You will join a dedicated highly renowned team offering you the opportunity to grow and develop your professional skills and profile You will have the opportunity to learn about building a startup business from experienced professionals and serial entrepreneurs Application Contact Information Your application should include a CV and cover letter highlighting your relevant experiences and motivations. Please send this to careers@pangaeadata.ai General Information Pangaea Data is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity or expression, religion, national origin or ancestry, age, disability, marital status, pregnancy, protected veteran status, protected genetic information, political affiliation, or any other characteristics protected by local laws, regulations, or ordinances."
4166208325,Data Engineer,Microsoft Power Platform Community,"Overview Do you have a passion for big data, analytics, insights, and technology? Do you want to be part of a team that enables end-to-end visibility, insights, and optimization, while doing good in the world? The Central Fraud Risk and Abuse (CFAR) organization is looking for a talented Data Engineer professional who wants to get their hands on large scale and complex data landscape, with deep knowledge of data architecture and management, and distributed systems, who is interested in working alongside Software Engineers to drive business impact. The Data Services team is leveraging data and modeling in new ways to increase visibility and transparency, improve insights and decisions, and optimize service levels, costs and customer experience. You will be joining a great team of awesome Data Engineers that move fast and innovate. You will help us build optimized data pipelines to scale from hundreds of millions of events per day to billions. If your toolbox includes proficiencies in Big Data, Azure Data Factory, Cloud Data Architecture, Data analytics and reporting, Synapse, Data Warehouse, SQL and MS Fabric we would love to apply your data expertise to have real world impact. Responsibilities Design, develop, and maintain data pipelines and models for real-time and batch ingestion, analysis, reporting, optimization, data collection at scale and reusable Design and develop application components and services to support data related processes, presentation, and consumption . Considers testability, monitoring, reliability, and maintainability to develop solutions for quality validation and continuous improvement. Collaborate with project managers, engineers, and business stakeholders to understand business and technical requirements, plan and execute projects, and communicate status, risks and issues. Perform root cause analysis of system and data issues and develop solutions as required. Use large data sets to provide insight into business and functional issues while improving data reliability, efficiency, and quality. Qualifications Required/Minimum Qualifications: Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 2+ years experience in business analytics, data science, software development, data modeling or data engineering work. OR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering or related field AND 1+ year(s) experience in business analytics, data science, software development, or data engineering work OR equivalent experience. 2+ years of experience in writing queries using SQL, KQL, Scope script. 2+ years of experience in developing solutions and implementing MS Fabric. 2+ years of experience with data cloud computing technologies such as – Azure Synapse, Azure Data Factory, SQL, Azure Data Explorer. Preferred/Additional Qualifications Microsoft Certified: Fabric Data Engineer Associate Experience building and optimizing large-scale data processing, integration and storage. Excellent analytical skills with a systematic and structured approach to software and data design. Experience with extracting and handling data with REST APIs, messaging and event technologies. 2+ years of experience in data engineering with proven coding and debugging skills in C#, Python, SQL, Spark, Scala. 2+ years of experience in developing solutions with Microsoft Power Platform, including Power BI, Fabric, Power Automate & M365 Dataverse. Data Engineering IC3 - The typical base pay range for this role across the U.S. is USD $98,300 - $193,200 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $127,200 - $208,800 per year. Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay Microsoft will accept applications for the role until March 12, 2025 Understanding and experience implementing Data Privacy and Security controls to protect the data. Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form . Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work."
4173971027,Data Engineer,"Global Soft Systems, Inc.","Title: Data Engineer Location: Hybrid in Bentonville, AR Must Have Skills Spark Kafka Integrating Data from multiple sources into one Data design, building data services, some warehousing Sql - creating structures, tables, building services around that R/ SSAS Azure Sql, google big query Requirement : Bachelor’s degree in Computer Science or related field or equivalent combination of industry-related professional experience and education."
4146947895,,,
4146869155,Data Engineer,GenBio AI,"Headquartered in Silicon Valley, we are a newly established start-up, where a collective of visionary scientists, engineers, and entrepreneurs are dedicated to transforming the landscape of biology and medicine through the power of Generative AI. Our team comprises leading minds and innovators in AI and Biological Science, pushing the boundaries of what is possible. We are dreamers who reimagine a new paradigm for biology and medicine. We are committed to decoding biology holistically and enabling the next generation of life-transforming solutions. As the first mover in pan-modal Large Biological Models (LBM), we are pioneering a new era of biomedicine, with our LBM training leading to ground-breaking advancements and a transformative approach to healthcare. Our exceptionally strong R&D team and leadership in LLM and generative AI position us at the forefront of this revolutionary field. With headquarters in Silicon Valley, California, and a branch office in Paris, we are poised to make a global impact. Join us as we embark on this journey to redefine the future of biology and medicine through the transformative power of Generative AI. Key Responsibilities: Design, develop, optimize, and maintain software systems for the entire foundation model development and deployment lifecycle (i.e., data pipeline, pre-training, fine-tuning, serving) Build and maintain scalable, efficient, and reusable codebases for large-scale foundation model training, adaptation, evaluation, and inference Collaborate closely with data engineers and research scientists to integrate models into production environments Implement and ensure best practices in software engineering, including code quality, testing, and documentation Build and optimize robust back-end systems, APIs, and databases to support complex workflows Ensure code quality, scalability, and performance through rigorous testing and code reviews Qualifications: Bachelor’s, Master’s degree in Computer Science, Engineering, or related field. Experience in life sciences or healthcare is a plus Strong programming skills in JavaScript, Python, and modern web development frameworks, and familiarity with GPU-accelerated tools (e.g., CUDA, cuDNN, Triton) Proficiency with major deep learning frameworks such as PyTorch, HuggingFace Transformers & Accelerate, or Megatron-LM/DeepSpeed Familiarity with resource management and scheduling systems (e.g., SLURM, Kubernetes) Proficiency in back-end frameworks like Django, Flask, or Node.js, and database technologies (e.g., PostgreSQL, MongoDB) Expertise in distributed systems, cloud computing (AWS, GCP), and containerization tools (Docker, Kubernetes) Preferred Qualifications: Ph.D. degree in Computer Science, Engineering, or related field. Experience in life sciences or healthcare is a plus Prior experience pre-training or serving large language models or large-scale foundation models Experience with deep learning workflows Knowledge of biological data types and challenges and experience with bioinformatics tools Familiarity with version control systems like Git and CI/CD pipelines Strong understanding of RESTful APIs, authentication, and deployment pipelines Familiarity with machine learning workflows and biological datasets Join us as we embark on this journey to redefine the future of biology and medicine. We are an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees."
4147808808,Staff Data Engineer - Samsung Ads,Samsung Electronics America,"Position Summary Samsung Ads is a leading innovator in advertising technology, dedicated to providing cutting-edge solutions that optimize ad performance and deliver exceptional results for our clients. We are seeking a highly skilled and experienced front-end staff engineer to join our dynamic team and help shape the future of the ad tech industry. We seek a talented Staff Data Engineer to play a pivotal role in enhancing our platform’s performance advertising capabilities. As an integral part of our engineering team, you will collaborate with cross-functional teams to design, implement, and optimize attribution models, ensuring accurate measurement of marketing campaign effectiveness. Role and Responsibilities Develop and implement advanced attribution models to analyze and attribute the impact of various marketing channels on user conversion. Collaborate with data scientists, product managers, and other engineers to refine and improve attribution methodologies. Design and maintain scalable and optimized data pipelines for efficient collection, processing, and storage of attribution-related data. Work closely with stakeholders to understand and translate business requirements into technical solutions. Conduct A/B testing and performance analysis to validate and iterate on attribution models. Stay updated on industry trends and emerging technologies related to attribution modeling and ad tech. Qualifications Bachelor’s or Master’s in Computer Science, Data Science, or a related field. Requires at least 8 years of related experience and a Bachelor's degree; or 6 years and a Master's degree; or a PhD with 3 years Proven experience in attribution modeling within the ad tech industry. Strong programming skills in Python, Java, or Scala. Experience in working with Kubernetes and stream data processing frameworks (Flink, Apache Ignite) Proficient in working with big data technologies and databases (e.g., Hadoop, Spark, SQL, and MapReduce). Hands-on experience with orchestration tools like Airflow or similar Solid understanding of statistical concepts and experience with relevant tools. Excellent problem-solving and communication skills. Preferred Qualifications Experience with machine learning techniques for attribution modeling. Familiarity with real-time data processing and streaming technologies. Knowledge of end-to-end digital advertising ecosystems and industry standards. Knowledge of Snowflake and related technologies California Only Compensation for this role is expected to be between $200,000 and $220,000. Actual pay will be determined considering factors such as relevant skills and experience, and comparison to other employees in the role. Please visit Samsung membership to see Privacy Policy, which defaults according to your location. You can change Country/Language at the bottom of the page. If you are European Economic Resident, please click here. At Samsung, we believe that innovation and growth are driven by an inclusive culture and a diverse workforce. We aim to create a global team where everyone belongs and has equal opportunities, inspiring our talent to be their true selves. Together, we are building a better tomorrow for our customers, partners, and communities. Samsung Electronics America, Inc. and its subsidiaries are committed to employing a diverse workforce, and provide Equal Employment Opportunity for all individuals regardless of race, color, religion, gender, age, national origin, marital status, sexual orientation, gender identity, status as a protected veteran, genetic information, status as a qualified individual with a disability, or any other characteristic protected by law. Reasonable Accommodations for Qualified Individuals with Disabilities During the Application Process Samsung Electronics America is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application process. If you have a disability and require a reasonable accommodation in order to participate in the application process, please contact our Reasonable Accommodation Team (855-557-3247) or SEA_Accommodations_Ext@sea.samsung.com for assistance. This number is for accommodation requests only and is not intended for general employment inquiries."
4163026121,Data Engineer,Flawless,"""The AI company that's revolutionizing Hollywood"" Flawless is shattering the boundaries of traditional filmmaking with its groundbreaking suite of Gen AI film editing tools. Our mission is to empower filmmakers with cutting-edge technology that allows creativity without compromise, expands storytelling possibilities, and delivers unparalleled visual and emotional experiences. We are also setting new standards in ethical AI by creating the Artistic Rights Treasury (A.R.T.), a rights management solution designed to protect artists and rights holders within the Entertainment landscape. Reports to: Software Engineering Manager What we are looking for: A Data Engineer, who is passionate about building platforms that massively reduce lead time from bringing Machine Learning research to production. The vision of the Data team is to enable our cross functional ML teams to spend most of their time solving tricky ML problems rather than dealing with data engineering/infra/ops challenges. You will impact the quality and accessibility of our data, working to source and catalogue high quality datasets and build automated annotation pipelines. You have a solid background in core software engineering principles, are happy deploying & managing infrastructure, and have a good understanding of the difficulties faced by research scientists and ML engineers. Responsibilities Building & maintaining data annotation pipelines, establishing the workflow for continuous data delivery and annotation Work with our ML teams to build data transformation pipelines for large scale computer vision datasets Establish robust data quality metrics and drive a continual improvement of data quality and diversity\ Drive standardisation of data management and exchange Support Flawless data sourcing efforts, license management and data governance strategy Working with our Platform team, supporting efficient data storage and transportation and access Qualifications Strong analytical background: BSc or MSc in data engineering/machine learning or related topics Programming experience with Python Experience creating and managing large scale datasets for machine learning, including establishing quality metrics. Specifically with unstructured video and image data Experience setting up infrastructure at scale for ML / Data Teams, including CI/CD & Data pipelines Experience working with cloud platforms (AWS, GCP, Azure) and familiarity with infrastructure as code Preferred Qualifications Experience within an early-stage data function, having played a key role in helping the data team develop into a more mature function during your tenure Experience working with the AWS Data Stack Experience working with large scale data, preferably in the Computer Vision domain Experience with multi-stage data transform pipelines, and large model training with 1000s of hours of video / image content Interview Process: At Flawless, our team and interview process want to help you show your best self. We’ll dive into past projects and simulate working together. Our interview process is four rounds with some casual Zoom (or in-person) coffee in between to get to know each other: Recruiting Screen: 30-45 minute call with our recruiting team (We want to discuss your background, interests and motivations as well as the practical details and make sure that Flawless would be a good fit for you) Coding Interview: - 60 min live coding test. This will be a technical paired-programming task with one of our engineering team This is designed to be an open/pairing type exercise to assess your ability to parse simple requirements, translate those into code, and demonstrate your fluency with hands on coding. Hiring Manager Screen: 45-60 minute zoom interview - you will meet with our Platform ENgineering Leadership to discuss your technical expertise Reverse System Design: 60 Minute zoom interview where you will meet 1-2 members of the Platform Engineering team, through a screen share format talking through experience in data transformation Final Behavioral Interview: 45 Minute onsite / virtual interview - Use this time to meet Sr Leaders in the Org, discussing cross functional teams, communication style, problem solving approach, and culture fit Your Recruiter and hiring manager will be your main point of contact and prepare you for interviews. You’ll meet 4 to 6 people from across the business. (We make sure that you have time in each interview to ask them questions). If we don’t give an offer, we’ll provide feedback! Why work at Flawless? You will be working in an environment based on trust, autonomy and collaboration, and this is a great opportunity for someone who wants to be part of a growing company in its most exciting stage of development. You can play a part in shaping the future of a company that’s caring, creative and collaborative. In addition to this, you'll also receive: Autonomy A hybrid working environment Competitive Salary Stock Options Flawless is proud to emphasize an equal opportunity, safe environment for people to do their best work. We are committed to providing equal employment opportunities regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements."
4113799942,Data Engineer,Flagler Health,"Our story We’re a fast growing, Series A stage healthare AI company building a clinical operating system for MSK practices. We are revenue generating and are looking to carefully expand the team to help us scale to 1,000,000 patients and beyond! The team is headquartered in NYC with a satellite office in Vancouver, BC. Much of our engineering team is remote - distributed throughout the US and Canada. We live in Notion docs, Slack channels and Github with weekly meetings over Zoom. Meetings may feature honorary advisors, MedTech execs, and, if we’re lucky, Will’s cat or our Chief Pup Officer. As Flagler continues to grow, there is a unique opportunity to build the foundations of data and infrastructure to help the product and company reach our full potential. This is where you come in — to design and build reliable, trusted, and timely analytics that accelerate the decision-making process of key product and business functions. You will have a strong impact on the roadmap and growth trajectory of our company. Key Responsibilities Databricks Platform Expertise Develop, manage, and optimize data pipelines on the Databricks platform. Debug and troubleshoot Spark applications to ensure reliability and performance. Implement best practices for Spark compute and optimize workloads. Python Development: Write clean, efficient, and reusable Python code using object-oriented programming principles. Design and build APIs to support data integration and application needs. Develop scripts and tools to automate data processing and workflows. MongoDB Management Integrate, query, and manage data within MongoDB. Ensure efficient storage and retrieval processes tailored to application requirements. Optimize MongoDB performance for large-scale data handling. Collaboration and Problem Solving: Work closely with data scientists, analysts, and other stakeholders to understand data needs and deliver solutions. Proactively identify and address technical challenges related to data processing and system design. Required Qualifications Proven experience working with Databricks and Spark compute. Proficient in Python, including object-oriented programming and API development. Familiarity with NoSQL (MongoDB preferred), including querying, data modeling, and optimization. Strong problem-solving skills and ability to debug and optimize data processing tasks. Experience with large-scale data processing and distributed systems. Preferred Qualifications Strong understanding of data architecture, ETL processes, and data warehousing concepts. Knowledge of other big data technologies like Delta Lake, Hadoop, or Kafka. Experience with cloud platforms (e.g., AWS, Azure, or GCP). Familiarity with CI/CD pipelines and version control systems like Git. Our values We spend at least as much time with our co-workers as we do with our closest friends + family; if we intend to do the most important and challenging work of our lives, it’s important that these folks energize us, support us, inspire us, and push us to do our best work. This is what you can expect of your teammates at Flagler: Persistence + ownership of outcomes: We wear many hats and aren’t afraid to run through walls to solve hard problems. Personal + professional growth: We push ourselves to learn new things and embrace challenges, even if it means that we sometimes fail. Don’t take things personally: We value and react quickly to constructive feedback. Speed is our ally: In the fast-paced world of startups, we understand the value of moving swiftly. We thrive on the adrenaline of working rapidly. Be Right: We are highly detailed oriented and try to be right, a lot. Hiring Process Due to the high volume of applications, we only reach out to candidates selected for interviews. We do not have online leetcode assessments as an initial filter, so we only reach out to very few candidates for initial introduction. Company Benefits We'll treat you well. If any other benefits are important to you, we'd love to know Competitive Salary & Meaningful Equity based on experience Unlimited PTO Health, dental, vision 401k Annual team offsite"
4170251261,Data Engineer,daydream,"Data Engineer (Growth) at daydream ✍🏻 About daydream daydream is a platform that automates programmatic SEO, end-to-end. This role is fully in-person in San Francisco. No remote work!*** The platform helps businesses identify search patterns, creates content at scale, and automatically improves that content. We've raised $6.2M from leading investors like First Round Capital, Basis Set Ventures, WndrCo, SOMA Capital and current and former Heads of Growth at companies like Notion, Airtable, Webflow and Dropbox as well as notable angels like Lenny Rachitsky and Eli Schwartz, Author of Product-Led SEO. Our customers include Descript, Clay, Beacons and a host of other venture-backed companies. We're on a high-growth trajectory and we're looking for people who want to join us on the journey to building a billion-dollar company. Our team previously worked together at Flixed, a startup we bootstrapped to $2M+ in revenue almost purely on the back of programmatic SEO. More reading: Our $3.8M seed raise announcement Our $2.5M pre-seed raise announcement Our feature in Axios Our founding story 🔧 The Problem we’re trying to solve Heads of Growth love programmatic SEO. It’s the secret growth playbook used by companies like Zapier, Canva, to acquire millions of visitors. The problem is, building a pSEO engine currently requires work from SEO analysts, content marketers, software engineers, data scientists and others. As a result, it’s logistically complicated and expensive. You can hire specialized agencies, but they cost $50K+ per month. 🧑‍💻 How we’re solving this problem daydream is building a SaaS platform that automates every piece of building a programmatic SEO engine. Everything from identifying search queries, to producing 90th percentile quality content at scale, all the way to automatically updating that content. 🪴 Why we’re hiring Demand for daydream is growing fast as our ARR continues to increase rapidly each month since the inception of our company, all before launching a generally available product. This is exciting, but it also means we need to hire to build product faster and meet the demand. We’re looking for a talent that’s excited to work with LLMs to serve one of the biggest markets in the world. 👋🏻 The People and Environment Here are three key elements of our culture: Push the pace No sandbagging allowed! Everyone at daydream is expected to proactively identify when a particular goal can be met faster, and to push for acceleration. A real example of how this value has been applied — we initially thought we would hit a certain revenue milestone within 1 year. We realized we can do it in half the time, so we shortened our timeline to hit that goal to 5 months instead. Build something you’re proud of Speed of execution is important, but only if it’s in service of delivering a delightful experience to the customer. We want you to look back at what you’ve built at daydream and feel a deep sense of satisfaction. A real example of how this value has been applied — In the past, we’ve prioritized “wowing” the customer at each release of daydream, even at the earliest phases. Speed is no excuse for delivering a poor experience. Five star customer service only The bar for customer service at daydream is always exceeding the customer’s expectations. It is not “meeting” the customer’s expectations. A real example of how this value has been applied — We’ve had situations where the customer is unintentionally moving slower than is possible with their SEO strategy. In these situations, we’ve proactively encouraged the customer to move faster. 🤝 The people you’ll be working with Thenuka (CEO and co-founder): Thenuka was previously the founder and CEO at Flixed, which he bootstrapped to $2M+ in revenue almost exclusively using programmatic SEO. Prior to that he was an Entrepreneur-in-Residence at Hustle Fund and Chief of Staff to the COO at Pango, a cybersecurity company with $100M+ in annual revenue. Shravan (CTO and co-founder): Shravan was previously an Engineering Lead at Flixed. Before that, Shravan worked as a Software Engineer at Meta for three years, where he worked on mass-scale web scraping for Facebooks’ Jobs product, and Meta Reality Labs. Nico (Growth Lead): When Nico is not working on daydream, he's writing for his newsletter, Failory, where he explores the reasons why startups fail. He started Failory when he was 15 years old and has since grown it to 40k subscribers. Devin (Operations Lead): Before daydream, Devin worked at an education startup called buildspace, an online alternative learning platform for people to work on their ideas they loved. There, he focused on building the community by helping produce and manage social media content, as well as hold in-person events across the world. 📝 What we're looking for Required Experience in web automation and data scraping. A strong work ethic, beginner's mind, and desire to learn quickly. Neat, organized, and punctual adherence to deadlines. A positive attitude and shared sense of camaraderie. Ideal Experience specializing in SEO is preferred but not required. 👷🏻‍♀️ What you’ll be doing here You’ll play a key role in building the foundation of our data-driven approach to programmatic SEO, working closely with customers and internal teams to create scalable, high-quality datasets. Ownership Areas Creative data collection: Identify and leverage APIs, web scraping techniques, and other automation methods to gather structured data that powers our programmatic SEO strategies. Data cleaning & normalization: Ensure data accuracy by cleaning, deduplicating, and structuring datasets in a way that makes them usable. Automation & efficiency: Develop reusable scripts and workflows to automate repetitive data collection and processing tasks, improving efficiency and scalability. 1 month Build context: Get to know our team, our customers, and the role data plays in driving our programmatic SEO strategies. Learn the product: Develop a deep understanding of how our product is used to scale data-driven content strategies. Begin data collection: Start identifying key datasets, experimenting with API connections, and setting up early-stage scraping workflows. 3 months Expand data collection capabilities: Identify new data sources and collaborate with customers and internal team to collect and structure data relevant to the different pSEO strategies. Refine & automate workflows: Develop reusable scripts and processes to streamline data collection, cleaning, and normalization. Collaborate on new use cases: Work with engineering and growth teams to explore additional ways data can drive innovation and efficiency across our platform. 6-9 months Support new hires: As a fast growing company, we will likely expand our team further in 6-9 months. You’ll help onboard new hires and help them get up to speed with daydream. 🏦 Compensation The compensation for this role is $110-$140K per year + generous equity in daydream. We usually prefer to have a full-time or part-time contract period before going straight to a full-time W-2 role."
4164137702,Data Engineer,LTIMindtree,"About Us: LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com. Job Title: Data Engineer Location: Cincinnati OH Job Summary: Skills Required Data Modeling Domain Driven Design Data Profiling and general Analysis Data Platforms Modernization Data Migration Emphasis on data management for OLTP systems but can help Data Warehouse team as well Technical Required ERD Logical Data Modeling tool Erwin or similar Databases and related Postgres Oracle Physical Data Model and architecture strong SQL skills stored procedures reading and understanding JSON Preferable Python Kafka Enterprise Elastic Search Bonus Data Services Rest API Hazelcast Java CRUD Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”): Benefits and Perks: Comprehensive Medical Plan Covering Medical, Dental, Vision Short Term and Long-Term Disability Coverage 401(k) Plan with Company match Life Insurance Vacation Time, Sick Leave, Paid Holidays Paid Paternity and Maternity Leave The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation. Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting. LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. Safe return to office : In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes."
4172390719,Data Engineer,Sabot Consulting,"Data Engineer – Childhood Support Division Location: Remote Salary: $90-115k Sabot Consulting is seeking a highly skilled Data Engineer that will work primarily with the Early Childhood and Family Support Division (ECFSD) Early Childhood Integrated Data System (ECIDS) effort to build a centralized data system/warehouse for unifying data within the division. This position will work closely with Division stakeholders to identify data requirements, onboard and transform data, confirm data quality and accessibility, and troubleshoot data-related technical issues. Responsibilities: Streamline data workflows and enhance data infrastructures for greater scalability and collaboration Identifies opportunities to optimize delivery by developing, maintaining, and testing infrastructure for data generation. Contribute to the management of all data assets, a data lake, and serves to help deliver modernized reporting and Business Intelligence solutions. Unify multiple disconnected data systems Design and implement a centralized data integration process using an IT Data Solution that integrates existing data from multiple systems and creates a unique identifier for programs to easily and accurately compare data. Where applicable, enable data sharing and interoperable data exchange for the Division, the Department, other state agencies, and external partners/providers. Oversees all stages of data onboarding, transformation, and enablement. Assembles complex data sets that meet non-functional and functional business requirements. Collaborates with associated programs to collect, prepare, organize, and secure the department’s data assets. Implements and enforces data privacy policies to comply with regulations. Provides data models and engages in data modeling efforts as needed. Delivers high-quality, uniform data to support analytics. Improves and streamlines access to data to drive innovation and outcomes. Prepares data for analytical and operational uses. Integrates, consolidates, and cleanses data and structures it for analytics applications. Must Have: 7+ years’ experience in data engineering and analytics, computer science, information technology, or related fields. Bachelor’s degree in Data Science from an accredited College or University 5+ years of experience building Snowflake data warehouse solutions incorporating multiple source system data 5+ Years integrating reporting solutions, specifically Thoughtspot, PowerBI, and/or Tableau. 5+ years of Data Modeling experience Significant experience producing deliverables on a regular cadence, as well as reporting on status to stakeholders Demonstrated ability to communicate effectively to diverse audiences with varying levels of technical understanding Nice to Have: Master’s Degree in a Data Science field is preferred Experience with Child Welfare systems and Data integration About Us: Sabot Consulting is a management consulting company focused on providing technical and management consulting to IT executives and managers in strategic, operational, and project-based practice areas. Our focus on providing expert staff that have the knowledge, experience, and professionalism to engage the client at all levels is the key to our success. Powered by JazzHR 9ZB0o54NLD"
4161048806,Data Engineer,Apexon,"Must Have : Develop/Maintain programs in SSIS/ Spark/ SQL Stored Procedures as part of ELT/ETL pipelines Experience with BI Tools like Tableau Proficient in writing SQL queries and data analysis Knowledge of relational database management systems like SQL Server/Snowflake Experience in programming languages like Java/Scala Great to Have: Experience with big data technologies like Hadoop, Spark, and Kafka Implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark Prior experience working in Banking/Finance domain Experience with Big Data Technologies - Spark (Java) for handling large-scale data processing Proficiency in SQL and Database - querying, managing, and manipulating data sets Knowledge of Cloud Platforms - data storage, processing, and deployment in a scalable environment (Azure)"
4168100133,Junior Data Engineer,Optimum Healthcare IT,"Entry Level Healthcare IT Analyst Start Your Career in Healthcare Information Technology Today! Getting your ﬁrst job can be difﬁcult when employers want experience, but to gain that experience, you need your ﬁrst job. We bridge the gap between your education and professional career by helping you gain the experience and training you need within the Healthcare Information Technology Industry. Optimum Healthcare IT is looking for recent college graduates with an interest in moving into the Healthcare IT Industry. Our Optimum CareerPath training program will equip you with the tools needed for your success as a Healthcare IT Analyst. Healthcare IT Analyst Job Responsibilities: · The Healthcare IT Analyst will have primary responsibility for the design, build/configuration, testing, validation, documentation, and ongoing support for the Healthcare applications. · This position will implement, administer, and support assigned systems under the guidance of senior members of the team. · The position will have a good understanding of healthcare organizations, ancillary systems, and health system operations. · Analyze and document user requirements, procedures, and problems to automate or improve existing systems. Review system capabilities, workflow, and scheduling limitations. · Document workflows, configure and/or build activities, change management adherence, end-user notifications, training information, and status reporting in the appropriate system. · Develop, document, and revise system design procedures, test procedures, and quality standards. · Expand or modify the system to serve new purposes or improve workflows. · Review and analyze the system and performance indicators to locate problems and correct errors. Escalate problems and issues to appropriate staff to ensure timely resolution. · Coordinate projects, and schedule and facilitate meetings as necessary to complete assignments. · Technical and functional analyst support of systems that may include Electronic Health Records platforms (Epic, Cerner), IT Project Management, ERP Systems (Workday, Oracle, PeopleSoft, UKG), ITSM applications (ServiceNow), data and analytics applications (Tableau, PowerBI), cloud deployments (GCP, Azure, AWS), and other digital platforms and services. Requirements: · Bachelor’s Degree · US work authorization (This position is not open to any H1B /F1 OPT/STEM degrees) · Excellent communication skills (verbal and written) · Ability to exercise tact and good interpersonal skills · Superb analytical and time management skills required · Self-starter, self-motivated, high level of initiative · Result-focused, ability to solve complex problems and resolve conflicts in a timely manner · Internships or Research Project Work is highly desired in a healthcare setting · Understanding of how data works and looks coming from different formats is preferred · Experience with SQL is a plus"
4167875770,Junior/Entry Level Data Engineer,TieTalent,"About Since 2010 and almost 14 years SynergisticIT has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Post Covid the tech Layoffs have been massive-In 2022 there were 165,269 tech layoffs, In 2023 there were 264,220 tech layoffs and so far in 2024 there have been 126,382 tech layoffs. (Source Layoffs.fyi ) -Total layoffs as per this 555,871 tech layoffs. Client, Dell and Cisco have announced 15,000/12,500 and 4000 tech layoffs respectively in August. The Job market is Hyper Competitive. For 1 position 500-2000 candidates or more are applying and laid off job seekers are also competing for entry level Job positions. Acquiring the right technology skillsets which are being demanded by clients and getting yourself in front of clients is the way to get to get Interviews and eventually a Job Offer. Survival of the Fittest is the only way to get a tech Job in this job market. In this Layoffs fueled market also SynergisticIT's candidates are able to achieve multiple job offers and $100k + salaries once they acquire the required skills. please check the below links to see success outcomes, salaries of our candidates . https://www.synergisticit.com/candidate-outcomes/ https://www.synergisticit.com/roi-of-computer-science-degree-colleges-vs-synergisticit/ We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 https://synergisticit.wistia.com/medias/tmwjwchxz5 https://synergisticit.wistia.com/medias/n8487768di https://synergisticit.wistia.com/medias/o5gmv7i9eu https://synergisticit.wistia.com/medias/k6t6a1n4kb https://synergisticit.wistia.com/medias/pgrvq4fgni https://synergisticit.wistia.com/medias/ce4syhm853 All Positions are open for all visas and US citizens We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few. Currently, we are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Data Engineers, Machine Learning engineers for full time positions with clients. Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry. We want Data Science/Machine learning/Data Analyst and Java Full stack candidates REQUIRED SKILLS For Java /Full stack/Devops Positions Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Knowledge of Core Java , javascript , C++ or software programming Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience For data Science/Data Analyst/AI/Machine learning Positions REQUIRED SKILLS Associate or Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT, Statistics, Mathematics or having good logical aptitude Knowledge of Statistics, Gen AI, LLM, Sagemaker, Python, Computer Vision, data visualization tools Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements. No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates Nice-to-have skills JavaScript C++ Arizona, United States"
4143931712,Data Engineer,TextNow,"We believe communication belongs to everyone. We exist to democratize phone service. TextNow is evolving the way the world connects, and that's because we're made up of people with curious minds who bring an optimistic yet critical lens into the work we do. We're the largest provider of free phone service in the nation. And we're just getting started. Join us in our mission to break down barriers to communication and free the flow of conversation for people everywhere. TextNow is looking for an experienced Data Engineer with hands-on experience designing and developing data platforms. You will own the design, development, and maintenance of TextNow's data platform , enabling us to make effective data-informed decisions. You will be part of cross-functional efforts to build scalable and reliable frameworks that support all of TextNow's business and data products. In this role, you can interact with different functional areas within the business and influence decision-making in a fast-growing mobile communications start-up. What You'll Do Own TextNow's data warehouse, pipeline, and integration points between various business systems. Explore available technologies and develop solutions to build and improve our identity resolution solutions. Design, Develop and support new and existing batch and real-time data pipelines and recommend improvements and modifications. Manage data to manage our AI/ML data products. Be a champion of TextNow's data ecosystem by working with engineering and infrastructure to implement data strategy for governance, security, privacy, quality, and retention that will satisfy business policies and requirements. Communicate strategies and processes around data modeling and architecture to cross-functional groups. Identify, design, and implement internal process improvements. Who You Are Have 3-8 years of experience working with data warehouse/data lake and ETL architectures, cloud data warehouses (Snowflake), and experience in Python and SQL, preferably at companies with fast-growing and evolving data needs. Have at least 1-2+ years of experience with Airflow, Iceberg, Spark and Flink. Exposure to AWS cloud data/ML services such as EKS, MWAA and Sagemaker. Developed scalable data pipelines using Python/Scala, SQL, and distributed processing frameworks like Spark or Flink. Experience driving technical vision for user identity resolution and data quality in previous roles is preferred. Hands on experience working with building data features using Snowflake, dbt, and Python to power real-time AI/ML inference. Respectfully candid with the ability to initiate and drive projects to completion. Highly organized, structured work approach and dependable. More about TextNow... Our Values Customer Obsessed (We strive to have a deep understanding of our customers) Do Right By Our People (We treat each other with fairness, respect, and integrity) Accept the Challenge (We adopt a ""Yes, We Can"" mindset to achieve ambitious goals) Act Like an Owner (We treat this company like it's our own... because it is!) Give a Damn! (We are deeply commited and passionate about our work and achieving results) Benefits, Culture, & More Strong work life blend Flexible work arrangements (wfh, remote, or access to one of our office spaces) Employee Stock Options Unlimited vacation Competitive pay and benefits Parental leave Benefits for both physical and mental well being (wellness credit and L&D credit) We travel a few times a year for various team events, company wide off-sites, and more Diversity And Inclusion At TextNow, our mission is built around inclusion and offering a service for EVERYONE, in an industry that traditionally only caters to the few who have the means to afford it. We believe that diversity of thought and inclusion of others promotes a greater feeling of belonging and higher levels of engagement. We know that if we work together, we can do amazing things, and that our differences are what make our product and company great. TextNow Candidate Policy By submitting an application to TextNow, you agree to the collection, use, and disclosure of your personal information in accordance with the TextNow Candidate Policy"
4158214840,Data Engineer,Auxilius,"Auxilius is a fast-growing, venture-backed B2B tech startup dedicated to helping innovative biotechs streamline their finance and accounting practices with a first-in-market software solution are looking for a Data Engineer to be a key member of our team! This role will be responsible for designing, developing, and maintaining our data infrastructure. You will build and optimize data pipelines, ensuring efficient data flow and system integration. Who are you? You are a problem solver at heart, driven by curiosity and a passion for building scalable, efficient data pipelines! You enjoy working with structured and unstructured data and transforming the raw information into meaningful insights. You thrive in a collaborative environment working closely with cross-functional teams to improve data accessibility and reliability, and you take pride in writing clean, maintainable code and optimizing data systems to support business growth! We’ll count on you to … Develop and maintain scalable data pipelines and ETL processes to support data science initiatives. Perform advanced analysis, data modeling, and toolset development to streamline key processes for Auxilius operations team and customers Collect and extract data from multiple sources, including databases, APIs, and external data providers, using data querying tools. Optimize database performance and storage solutions while ensuring compliance with data security regulations. Prepare clear and concise reports, visualizations, and dashboards to present analytical findings to stakeholders and senior management. Develop and implement data management strategy in alignment with Auxilius’s product development goals and customer implementation requirements Implement and maintain data quality monitoring, governance and documentation to ensure they are up to Auxilius and our customers’ standards Perform advanced statistical analysis and data modeling to support various business functions. Stay abreast of industry trends and advancements in data science, machine learning, and analytics to continually improve processes and systems. Preferred Skills and Qualifications A bachelor’s degree in data science, statistics, computer science, business analytics, or a related field is preferred. 5+ years of experience in data analysis, reporting, or a similar role, preferably within a startup or fast-paced environment. Strong experience with data analysis languages such as Python or R is required Strong experience with SQL for data querying, data modeling, mining, and visualization. Experience in automating data analysis processes and creating real-time dashboard reports Strong analytical and problem-solving skills with the ability to think outside the box, interpret complex data, and present it clearly and concisely. Self-motivated individual with a strong sense of ownership and the ability to work independently. Excellent verbal and written communication skills, with the ability to articulate ideas, collaborate effectively, and present information to both technical and non-technical audiences. High level of accuracy and attention to detail in all aspects of work A sense of humor! Who are we? Auxilius built data-driven, clinical trial financial management software for Biotechs. We help clinical trial Sponsors take control over trial costs, manage financial risk, and optimize spending in pursuit of trial outcomes. While our product looks like software, we are providing data, context, and frameworks that help Sponsors make decisions to achieve trial outcomes dynamically and cost-effectively — empowering teams of PhDs to bring drugs to market faster. Auxilius was founded by senior executives from the healthcare information and financial services industries. The founding team has spent almost a decade building workflow and intelligence tools for the Life Sciences industry by infusing data into intuitive SaaS solutions. We are backed by top-tier investors and advised by industry leaders."
4130662149,Data Engineer,Imprint,"Who We Are Imprint is building a next-generation co-branded credit card company to serve America’s great brands. Some of our partners include H-E-B, Turkish Airlines, Brooks Brothers, and Eddie Bauer. Imprint is backed by Khosla Ventures, Kleiner Perkins, and Thrive Capital. We are focused on building a brilliant team who want to change payments and who embody our Operating Principles. The Team The Data Analytics team at Imprint is dedicated to building a robust data foundation that drives smarter decision-making across the organization. The team is responsible for developing and maintaining Imprint’s data infrastructure, which includes not only the data warehouse and data pipelines but also comprehensive analytics systems that support both daily operations and strategic initiatives. By efficiently managing data flows, ensuring data quality, and optimizing performance, the Data Analytics team enables accessible, high-quality data that fuels insights into customer behavior, operational efficiency, and market trends. Leveraging these actionable insights, the team plays a crucial role in guiding Imprint's growth and profitability, empowering cross-functional teams to make well-informed, data-driven decisions. Your Day-to-Day Build and Scale Robust Data Pipelines Design, develop, and maintain scalable and efficient data pipelines, integrating data from diverse sources into our systems. Ensure data is ingested, transformed, and made available in a reliable, timely, and structured manner. Implement best practices for data orchestration, monitoring, and automation to optimize data flow. Architect and Optimize Data Models Collaborate with stakeholders to define and implement well-structured data models that enable analytics and reporting. Optimize database schema, partitioning, and indexing strategies to ensure high performance and query efficiency. Drive data governance efforts to ensure data consistency, lineage, and usability. Ensure High-Quality, Trustworthy Data Establish robust data validation, monitoring, and anomaly detection mechanisms to ensure data integrity and reliability. Proactively troubleshoot and resolve data quality issues, implementing automated checks and alerts. Continuously enhance data observability to minimize downtime and improve system resilience. Optimize Data Storage and Performance Design and manage cloud-based data storage solutions, optimizing for scalability, cost efficiency, and query performance. Leverage distributed computing and modern data architectures to enhance processing efficiency. Continuously tune data infrastructure for optimal performance, ensuring minimal latency and maximum throughput. Drive Cross-Team Collaboration and Innovation Work closely with data scientists, analysts, and business teams to understand and support their data needs. Provide guidance on best practices for data access, transformation, and analytics. Champion the adoption of new technologies and methodologies to advance our data engineering capabilities. We Are Looking For Folks With 4+ years of experience working in data architecture, data pipeline, data warehouse modeling, master data management Expertise with (Must have hands-on experience) A full modern data stack (Fivetran / Snowflake / dbt / Airflow / AWS) SQL, dbt, Python Github, Terraform, CI/CD Experience designing, developing, and maintaining large-scale and well-documented data marts to drive data insights Excellent written and verbal communication and interpersonal skills, and ability to effectively collaborate with technical and business teams Bonus Points Experience in banking, financial services, credit card, fintech in a start-up environment Perks & Benefits Competitive compensation and equity packages Leading configured work computers of your choice Unlimited vacation policy Fully covered, high-quality healthcare including fully covered dependent coverage Additional health coverage includes access to One Medical and option to enroll in an FSA 16 weeks of paid parental leave for the primary caregiver and 8 weeks for all new parents An understanding that successful remote work requires flexibility and an appreciation for asynchronous work Access to industry leading technology across all of our business units — stemming from our philosophy that we should invest in resources for our team that foster innovation, optimization, and productivity Imprint is committed to a diverse and inclusive workplace. Imprint is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. Imprint welcomes talented individuals from all backgrounds who want to build the future of payments and rewards. If you are passionate about FinTech and eager to grow, let’s move the world forward, together. Compensation Range: $120K - $180K"
